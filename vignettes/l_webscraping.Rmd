---
title: "Grundlagen des Webscraping"
author: "Andreas Blaette"
date: "09. Juli 2015"
output: html_document
vignette: >
  %\VignetteIndexEntry{12 - Grundlagen des Webscraping}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}  
---


Einfacher Download von Dateien aus dem Internet
===============================================

direktes Einlesen von Tabellen (csv-Format)
-------------------------------------------

** Unser altes Beispiel: Wahlkreisergebnisse **

```{r, eval = FALSE}
urlWahlkreisergebnisse <- "http://www.bundeswahlleiter.de/de/bundestagswahlen/BTW_BUND_13/veroeffentlichungen/ergebnisse/kerg.csv"
wkTab <- read.table(
  urlWahlkreisergebnisse, header=T, skip=2, fileEncoding="ISO-8859-1", sep=";"
)
wkTab <- wkTab[c(3:nrow(wkTab)),]
```

** Daten von www.govdata.de **

... wenn man nach NRW sucht, findet man ...

```{r}
wahlenKoelnUrl <- "http://offenedaten-koeln.de/sites/default/files/ergebnisse_der_kommunalwahlen_in_koeln_seit_1946_prozent.csv"
koelnTab <- read.table(wahlenKoelnUrl, header=T, sep=",")
```

** Daten von Eurostat **

```{r, eval = FALSE}
library("eurostat")
eurostatId <- "tps00189" # asylum and new asylum applicants
asylum <- get_eurostat(eurostatId)
is(asylum)
dim(asylum)
View(asylum)
```

Aha! Wir bekommen die Daten in extensiver Form. Kennen wir von ggplot ...


Download und Speichern von Dateien
==================================

```{r, eval = FALSE}
btArchive <- "http://dipbt.bundestag.de/doc/btp/"
lp <- 18
lpArchive <- paste(btArchive, as.character(lp), sep="")
maxDoc <- 116
downloadDir <- "/Users/blaette/Lab/tmp/btDownload"
for (docNo in c(1:maxDoc)){
  docNoFormatted <- sprintf("%03d", docNo)
  print(docNoFormatted)
  pdfFilename <- paste(lp, docNoFormatted, ".pdf", sep="")
  download.file(
    url=paste(lpArchive, "/", lp, docNoFormatted, ".pdf", sep=""),
    destfile=file.path(downloadDir, pdfFilename)
  )
}
```


Einlesen von html-Tabellen
==========================

```{r, eval = FALSE}
library(RCurl)
library(XML)
wikipediaUrl <- "https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_%2818._Wahlperiode%29"
wikipediaHtml <-  RCurl::getURL(wikipediaUrl)
wikipediaXml <- htmlTreeParse(wikipediaHtml, useInternalNodes=TRUE)
tableXml <- getNodeSet(wikipediaXml, '//table[@class="wikitable"]')[[1]]
tableXml <- getNodeSet(wikipediaXml, '//table')[[2]]
table <- readHTMLTable(tableXml)

```

```{r, eval = FALSE}
library(rvest)
wikipediaUrl <- "https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_%2816._Wahlperiode%29"
page <- html(wikipediaUrl)
tables <- html_nodes(page, "table")
tab <- html_table(tables[[1]])
```

Webscraping 2 
==============
```{r, eval = FALSE}
pm <- "http://www.bundesregierung.de/Content/DE/Pressemitteilungen/BPA/2015/07/2015-07-09-bkm-kulturelle-bildung.html"
page <- html(pm)
dateNode <- html_nodes(page, xpath='//div[@class="abstract"]')
pClass <- dateNode <- html_nodes(page, css="LMFuss")
html_text(pClass)
abstractText <- html_text(dateNode[[1]])
```

```{r, eval = FALSE}
faz <- "http://www.faz.net/aktuell/politik/inland/die-steuer-die-spaltet-13692526.html"
fazPage <- html(faz)
linkedComments <- html_nodes(fazPage, "LMFuss")
```


```{r}
if (!"twitteR" %in% rownames(installed.packages())){
  install.packages("twitteR")
}
library(twitteR)
```


```{r setup, include=FALSE, eval = FALSE}
setwd("~/Lab/analysen/twitter")
api_key <- scan(file = "api_key.txt", what = character())
api_secret <- scan(file = "api_secret.txt", what = character())
access_token <- scan(file = "access_token.txt", what = character())
access_token_secret <- scan(file = "access_token_secret.txt", what = character())
```

```{r, eval = FALSE}
setup_twitter_oauth(
  consumer_key = api_key,
  consumer_secret = api_secret,
  access_token = access_token,
  access_secret = access_token_secret
  )
```

```{r, eval = FALSE}
tweet_list <- list()
dates <- seq(from = as.Date("2017-06-01"), to = as.Date("2017-07-14"), by = "day")

for (i in 2:length(dates)){
  print(dates[i])
  twitter_raw <- searchTwitter(
    searchString = "#pulseofeurope+#PulseOfEurope", # n = 1000,
    since = as.character(dates[i-1]),
    until = as.character(dates[i])
  )
  tweet_list[[as.character(dates[i])]] <- data.frame(
    text = sapply(twitter_raw, function(x) x$getText()),
    created = sapply(twitter_raw, function(x) as.character(as.Date(x$getCreated()))),
    screen_name = sapply(twitter_raw, function(x) x$getScreenName()),
    retweet = sapply(twitter_raw, function(x) x$getRetweeted()),
    date = rep(as.character(dates[i]), times = length(twitter_raw))
  )
}
tweets_df <- do.call(rbind, tweet_list)
# tweets_df[["date"]] <- gsub("^(\\d{4}-\\d{2}-\\d{2}).*$", "\\1", # rownames(tweets_df))
```

```{r, eval = FALSE}
unique(tweets_df[, "created"])
unique(tweets_df[, "date"])
```


```{r, eval = FALSE}
dt <- as.data.table(tweets_df)
dt_n <- dt[, .N, by = c("screen_name", "date")]

for_gg <- as.data.frame(dt_n[1:10])

d <- ggplot(data = for_gg, aes(x = screen_name, y = N))
d <- d + geom_bar(stat = "identity", position = "dodge")
d <- d + xlab("Urheber")
d <- d + ylab("Zahl der tweets")
d


dt_cross <- dcast(
  data = dt_n,
  formula = date ~ screen_name,
  value.var = "N"
  )
colSums(dt_cross[,2:ncol(dt_cross)])

Z <- zoo(dt_cross[,2:ncol(dt_cross)], order.by = as.Date(dt_n[["date"]]))

plot(as.xts(Z[, "schwar_ron"]))
```